{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: AWS Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'project-thunderbirds-storage'\n",
    "prefix = 'Scikit-LinearLearner-pipeline-disaster-tweets'\n",
    "DATA_DIR = 'data'\n",
    "prefix = 'sagemaker-template-test'\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Data\n",
    "\n",
    "You may want to be able to do a few different things with datasets:\n",
    " - Use local data (either before uploading to S3 or for local sessions)\n",
    " - Upload the local data to S3\n",
    " - Use data already uploaded on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Local Data : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11905</th>\n",
       "      <td>screwed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25206</th>\n",
       "      <td>wondering earthquake haiti would put unsalted ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6543</th>\n",
       "      <td>crazy ideas album im soooo tired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15267</th>\n",
       "      <td>approximately persons camping outside roadside</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27053</th>\n",
       "      <td>rawalpindi nov app governor punjab lt gen r kh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    data  label\n",
       "11905                                            screwed      0\n",
       "25206  wondering earthquake haiti would put unsalted ...      1\n",
       "6543                    crazy ideas album im soooo tired      0\n",
       "15267     approximately persons camping outside roadside      1\n",
       "27053  rawalpindi nov app governor punjab lt gen r kh...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv('data/clean_train.csv')\n",
    "train_dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Local Data: Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3279</th>\n",
       "      <td>Essex, Massachusetts</td>\n",
       "      <td>bane le babylonian breaking back one chain bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>Atlantic City, New Jersey</td>\n",
       "      <td>folks big storm key food http co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>Monmouth County, New Jersey</td>\n",
       "      <td>cowanjacob hate jumped front car would hit bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>Cape May, New Jersey</td>\n",
       "      <td>done single hurricane sandy joke till might lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8963</th>\n",
       "      <td>Cape May, New Jersey</td>\n",
       "      <td>every season praise jah barbados usual path hu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         location  \\\n",
       "3279         Essex, Massachusetts   \n",
       "2434    Atlantic City, New Jersey   \n",
       "4096  Monmouth County, New Jersey   \n",
       "7530         Cape May, New Jersey   \n",
       "8963         Cape May, New Jersey   \n",
       "\n",
       "                                                   data  \n",
       "3279  bane le babylonian breaking back one chain bru...  \n",
       "2434                   folks big storm key food http co  \n",
       "4096  cowanjacob hate jumped front car would hit bra...  \n",
       "7530  done single hurricane sandy joke till might lo...  \n",
       "8963  every season praise jah barbados usual path hu...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dataset = pd.read_csv('data/validation_dataset_utf8_location_and_message.csv')\n",
    "batch_dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-eu-west-2-663466646878\n",
      "Data: s3://project-thunderbirds-storage/sagemaker-template-test/train/clean_train.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset.to_csv(\"data/clean_train.csv\", index=False)\n",
    "\n",
    "# We uploading the /data folder to a bucket\n",
    "# data = sagemaker_session.upload_data(DATA_DIR, key_prefix=\"{}/{}\".format(prefix, DATA_DIR))\n",
    "\n",
    "### UPLOAD DATA TO S3 FOR PASSING TO MODEL LATER ON ###\n",
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "train_data = sagemaker_session.upload_data(\n",
    "    path='{}/{}'.format(WORK_DIRECTORY, 'clean_train.csv'), \n",
    "    bucket=s3_bucket,\n",
    "    key_prefix='{}/{}'.format(prefix, 'train'))\n",
    "\n",
    "print (\"Bucket: \" + sagemaker_session.default_bucket())\n",
    "print (\"Data: \" + train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-eu-west-2-663466646878\n",
      "Data: s3://project-thunderbirds-storage/sagemaker-template-test/batch_input/batch.csv\n"
     ]
    }
   ],
   "source": [
    "batch_dataset.to_csv(\"data/batch.csv\", index=False)\n",
    "\n",
    "# We uploading the /data folder to a bucket\n",
    "# data = sagemaker_session.upload_data(DATA_DIR, key_prefix=\"{}/{}\".format(prefix, DATA_DIR))\n",
    "\n",
    "### UPLOAD DATA TO S3 FOR PASSING TO MODEL LATER ON ###\n",
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "batch_data = sagemaker_session.upload_data(\n",
    "    path='{}/{}'.format(WORK_DIRECTORY, 'batch.csv'), \n",
    "    bucket=s3_bucket,\n",
    "    key_prefix='{}/{}'.format(prefix, 'batch_input'))\n",
    "\n",
    "print (\"Bucket: \" + sagemaker_session.default_bucket())\n",
    "print (\"Data: \" + batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Test Data Downloaded Successfully from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>got enough candles supply mexican family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>sandy soooo mad shattering doors shiet hurrica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>taos never got magnificent case burgundy sent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>mad river bar amp grille new york ny http co v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>sandy weak name hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>seaoftime freaking excited know plans hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>rt find hurricane jokes funny itsnotajoke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>best wishes friends northeast stay safe hurric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>update threat hurricane sandy grows targets us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10008 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      location  \\\n",
       "0      New York City, New York   \n",
       "1      New York City, New York   \n",
       "2      New York City, New York   \n",
       "3      New York City, New York   \n",
       "4      New York City, New York   \n",
       "...                        ...   \n",
       "10003  New York City, New York   \n",
       "10004  New York City, New York   \n",
       "10005  New York City, New York   \n",
       "10006  New York City, New York   \n",
       "10007  New York City, New York   \n",
       "\n",
       "                                                    data  \n",
       "0               got enough candles supply mexican family  \n",
       "1      sandy soooo mad shattering doors shiet hurrica...  \n",
       "2      ibexgirl thankfully hurricane waugh played coo...  \n",
       "3      taos never got magnificent case burgundy sent ...  \n",
       "4      mad river bar amp grille new york ny http co v...  \n",
       "...                                                  ...  \n",
       "10003                          sandy weak name hurricane  \n",
       "10004    seaoftime freaking excited know plans hurricane  \n",
       "10005          rt find hurricane jokes funny itsnotajoke  \n",
       "10006  best wishes friends northeast stay safe hurric...  \n",
       "10007  update threat hurricane sandy grows targets us...  \n",
       "\n",
       "[10008 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Testing batch_input\n",
    "pd.read_csv(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access S3 Data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = session.client('s3')\n",
    "# # [Bucket] [Object/Folder/File-on-s3] [local-file-name-to-be-saved]\n",
    "# s3.download_file('sagemaker-us-west-2-991775080983', 'sagemaker-template-test/data/reviews.csv', 's3_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Entry Point File\n",
    "\n",
    "This cell will produce a python file, which is the source code for our 'Estimator' and will be passed in as a parameter below. This is where all the AI code should go. This means it need its own imports - even if they are duplicates :/\n",
    "\n",
    "We are required to define four methods:\n",
    "\n",
    " - model_fn: Return the model from S3 location and prep it for inference\n",
    " - input_fn: Take the HTTP request and translate for the prediction code\n",
    " - predict_fn: Make predictions using the formatted input and model\n",
    " - output_fn: Take the prediciton and get it ready to return as the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EstimatorScript.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EstimatorScript.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "import joblib \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sagemaker_containers.beta.framework import (worker, encoders)\n",
    "\n",
    "# method to train and save the model\n",
    "def training():\n",
    "    \n",
    "    print(\"TRAINING FUNCTION START\")\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "\n",
    "    raw_data = [ pd.read_csv(file, engine=\"python\") for file in input_files ]\n",
    "    train_data = pd.concat(raw_data)\n",
    "    \n",
    "    # labels are in the first column\n",
    "    train_y = train_data['label']\n",
    "    print(str(train_y.sample(5)))\n",
    "    train_X = train_data['data']\n",
    "    print(str(train_X.sample(5)))\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(train_X.values.astype('U').tolist(), train_y.tolist())\n",
    "\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(pipeline, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(\"MAIN FUNCTION START\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    training()\n",
    "\n",
    "# [REQUIRED]: method to load model\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    print(\"MODEL FUNCTION START\")\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    \n",
    "    return clf\n",
    "\n",
    "# [REQUIRED]: method to handle input of requests....\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \n",
    "    print(\"INPUT FUNCTION START\") \n",
    "    print(str(request_content_type))\n",
    "    \"\"\"An input_fn that loads a pickled numpy array\"\"\"\n",
    "    \n",
    "    if request_content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        print('CSV FILE INPUT RECEIVED')\n",
    "        df = pd.read_csv(StringIO(request_body), dtype={\"data\": \"string\"})\n",
    "        print(str(df.head(3)))\n",
    "        print(df.dtypes)\n",
    "        print(df.shape)       \n",
    "        return df\n",
    "    elif request_content_type == \"application/python-pickle\":\n",
    "        array = np.load(BytesIO((request_body)))\n",
    "        return array\n",
    "    elif request_content_type == 'application/json':\n",
    "        jsondata = json.load(StringIO(request_body))\n",
    "        #processed_data = process_data(jsondata)\n",
    "        return [jsondata['instances'][0]['features'][0]]\n",
    "    else:\n",
    "        # Handle other content-types here or raise an Exception\n",
    "        # if the content type is not supported.\n",
    "        raise ValueError(\"{} not supported by script!\".format(request_content_type))\n",
    "\n",
    "# [REQUIRED]: method to run prediction on the model\n",
    "def predict_fn(input_data, model):\n",
    "    \n",
    "    print(\"PREDICT FUNCTION START\")\n",
    "    print(str(input_data.head(3)))\n",
    "    print(type(input_data))\n",
    "    print(input_data.dtypes)\n",
    "    print(input_data.shape) \n",
    "    predictions = []\n",
    "    \n",
    "    for index, row in input_data.iterrows():\n",
    "        message=[row['data']]\n",
    "#       print(type(message)) statement returns <class 'list'>\n",
    "        prediction = model.predict(message)\n",
    "        predictions.append(prediction[0])\n",
    "        \n",
    "    print(predictions[:10])\n",
    "    nrow = len(predictions)\n",
    "    print('Predictions list number of rows:')\n",
    "    print(str(nrow))\n",
    "    print(type(predictions)) # statement returns <class 'list'>\n",
    "    predictions.insert(0, 0)\n",
    "    \n",
    "    return predictions \n",
    "\n",
    "# [REQUIRED]: method to handle output of requests....\n",
    "def output_fn(prediction, accept):\n",
    "    \n",
    "    print(\"OUTPUT FUNCTION START\")\n",
    "    print(prediction[:10])\n",
    "    print(type(prediction)) \n",
    "    nrow = len(prediction)\n",
    "    print('Prediction list number of rows:')\n",
    "    print(str(nrow))\n",
    "    \n",
    "    if accept == \"application/json\":\n",
    "        return worker.Response(json.dumps(prediction), accept, mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        output = worker.Response(encoders.encode(prediction, accept), accept, mimetype=accept)\n",
    "        print(type(output))\n",
    "        print(str(output))\n",
    "        return output\n",
    "    else:\n",
    "        raise ValueError(\"{} accept type is not supported by this script.\".format(accept)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Estimator\n",
    "\n",
    "Sagemaker object that takes the above python file as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = 'EstimatorScript.py'\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    role=role,\n",
    "    framework_version='0.23-1',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-05 13:24:32 Starting - Starting the training job...\n",
      "2021-03-05 13:24:57 Starting - Launching requested ML instancesProfilerReport-1614950672: InProgress\n",
      "......\n",
      "2021-03-05 13:25:58 Starting - Preparing the instances for training...\n",
      "2021-03-05 13:26:18 Downloading - Downloading input data...\n",
      "2021-03-05 13:26:59 Training - Training image download completed. Training in progress..\u001b[34m2021-03-05 13:26:59,496 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2021-03-05 13:26:59,498 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-05 13:26:59,506 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-03-05 13:26:59,822 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-05 13:27:02,848 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-05 13:27:02,859 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-05 13:27:02,867 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2021-03-05-13-24-32-365\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-2-663466646878/sagemaker-scikit-learn-2021-03-05-13-24-32-365/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"EstimatorScript\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"EstimatorScript.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=EstimatorScript.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=EstimatorScript\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-2-663466646878/sagemaker-scikit-learn-2021-03-05-13-24-32-365/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2021-03-05-13-24-32-365\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-2-663466646878/sagemaker-scikit-learn-2021-03-05-13-24-32-365/source/sourcedir.tar.gz\",\"module_name\":\"EstimatorScript\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"EstimatorScript.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python EstimatorScript.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mMAIN FUNCTION START\u001b[0m\n",
      "\u001b[34mTRAINING FUNCTION START\u001b[0m\n",
      "\u001b[34m11219    0\u001b[0m\n",
      "\u001b[34m28469    0\u001b[0m\n",
      "\u001b[34m19301    0\u001b[0m\n",
      "\u001b[34m31830    0\u001b[0m\n",
      "\u001b[34m3481     0\u001b[0m\n",
      "\u001b[34mName: label, dtype: int64\u001b[0m\n",
      "\u001b[34m9984     raising money hurricane harvey disaster housto...\u001b[0m\n",
      "\u001b[34m30759                        trying figure twitter dnt get\u001b[0m\n",
      "\u001b[34m23472    also runs drug rehabilitation assistance progr...\u001b[0m\n",
      "\u001b[34m31761    help distribute supplies food great organizing...\u001b[0m\n",
      "\u001b[34m20002    slrcs federation american relief emergency res...\u001b[0m\n",
      "\u001b[34mName: data, dtype: object\u001b[0m\n",
      "\u001b[34m2021-03-05 13:27:07,068 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-03-05 13:27:19 Uploading - Uploading generated training model\n",
      "2021-03-05 13:27:19 Completed - Training job completed\n",
      "Training seconds: 59\n",
      "Billable seconds: 59\n"
     ]
    }
   ],
   "source": [
    "sklearn_estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# batch_file = 'batch.csv'\n",
    "# sm_transformer = sklearn_estimator.transformer(1, 'ml.m5.xlarge', assemble_with = 'Line', accept = 'text/csv')\n",
    "# ###  Batch Transformer that will call the model endpoint ###\n",
    "# # start a transform job\n",
    "# input_location = 's3://{}/{}/batch_input/{}'.format(s3_bucket, prefix, batch_file )# use input data with Location column\n",
    "# sm_transformer.transform(input_location, split_type='Line', content_type='text/csv', \n",
    "#                          input_filter='$[1:]', join_source='Input', output_filter='$[0,-1]')\n",
    "# sm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sm_transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data returned from Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to return the data from the Batch Transform Job ###\n",
    "\n",
    "# import json\n",
    "# import io\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# def get_csv_output_from_s3(s3uri, file_name):\n",
    "#     parsed_url = urlparse(s3uri)\n",
    "#     bucket_name = parsed_url.netloc\n",
    "#     prefix = parsed_url.path[1:]\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "#     return obj.get()[\"Body\"].read().decode('utf-8')\n",
    "\n",
    "# output = get_csv_output_from_s3(sm_transformer.output_path, '{}.out'.format(batch_file))\n",
    "# output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=0)\n",
    "# output_df.shape\n",
    "# print(output_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Logic to identify if a disaster is happening (will reside in lambda) ###\n",
    "\n",
    "# import numpy as np\n",
    "# predictionsArray = np.array([])\n",
    "# for index, row in output_df.iterrows():\n",
    "#         result=[row['0']]\n",
    "# #       print(type(message)) statement returns <class 'list'>\n",
    "#         predictionsArray = np.append(predictionsArray, result)\n",
    "# disasterRelatedTweets = 0\n",
    "# for prediction in predictionsArray:\n",
    "#     if (prediction == 1):\n",
    "#         disasterRelatedTweets += 1;\n",
    "# if(disasterRelatedTweets >=3000):\n",
    "#     print (\"A disaster is currently happening! \")\n",
    "#     print (disasterRelatedTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  output_df.to_csv(\"data/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn_estimator.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\", \n",
    "                                     endpoint_name='natural-disaster-detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.delete_endpoint(EndpointName=natural-disaster-detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
